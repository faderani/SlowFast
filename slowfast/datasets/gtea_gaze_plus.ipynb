{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-2175e54df429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mslowfast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDATASET_REGISTRY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/soroush/Projects/slowfast/slowfast/datasets/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/soroush/Projects/slowfast/slowfast/datasets/transform.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrand_augment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrand_augment_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandom_erasing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomErasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import random\n",
    "from itertools import chain as chain\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from iopath.common.file_io import g_pathmgr\n",
    "\n",
    "import slowfast.utils.logging as logging\n",
    "\n",
    "import utils as utils\n",
    "from .build import DATASET_REGISTRY\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "@DATASET_REGISTRY.register()\n",
    "class Charades(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Charades video loader. Construct the Charades video loader, then sample\n",
    "    clips from the videos. For training and validation, a single clip is randomly\n",
    "    sampled from every video with random cropping, scaling, and flipping. For\n",
    "    testing, multiple clips are uniformaly sampled from every video with uniform\n",
    "    cropping. For uniform cropping, we take the left, center, and right crop if\n",
    "    the width is larger than height, or take top, center, and bottom crop if the\n",
    "    height is larger than the width.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, mode, num_retries=10):\n",
    "        \"\"\"\n",
    "        Load Charades data (frame paths, labels, etc. ) to a given Dataset object.\n",
    "        The dataset could be downloaded from Chrades official website\n",
    "        (https://allenai.org/plato/charades/).\n",
    "        Please see datasets/DATASET.md for more information about the data format.\n",
    "        Args:\n",
    "            dataset (Dataset): a Dataset object to load Charades data to.\n",
    "            mode (string): 'train', 'val', or 'test'.\n",
    "        Args:\n",
    "            cfg (CfgNode): configs.\n",
    "            mode (string): Options includes `train`, `val`, or `test` mode.\n",
    "                For the train and val mode, the data loader will take data\n",
    "                from the train or val set, and sample one clip per video.\n",
    "                For the test mode, the data loader will take data from test set,\n",
    "                and sample multiple clips per video.\n",
    "            num_retries (int): number of retries.\n",
    "        \"\"\"\n",
    "        # Only support train, val, and test mode.\n",
    "        assert mode in [\n",
    "            \"train\",\n",
    "            \"val\",\n",
    "            \"test\",\n",
    "        ], \"Split '{}' not supported for Charades \".format(mode)\n",
    "        self.mode = mode\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self._video_meta = {}\n",
    "        self._num_retries = num_retries\n",
    "        # For training or validation mode, one single clip is sampled from every\n",
    "        # video. For testing, NUM_ENSEMBLE_VIEWS clips are sampled from every\n",
    "        # video. For every clip, NUM_SPATIAL_CROPS is cropped spatially from\n",
    "        # the frames.\n",
    "        if self.mode in [\"train\", \"val\"]:\n",
    "            self._num_clips = 1\n",
    "        elif self.mode in [\"test\"]:\n",
    "            self._num_clips = (\n",
    "                cfg.TEST.NUM_ENSEMBLE_VIEWS * cfg.TEST.NUM_SPATIAL_CROPS\n",
    "            )\n",
    "\n",
    "        logger.info(\"Constructing Charades {}...\".format(mode))\n",
    "        self._construct_loader()\n",
    "\n",
    "    def _construct_loader(self):\n",
    "        \"\"\"\n",
    "        Construct the video loader.\n",
    "        \"\"\"\n",
    "        path_to_file = os.path.join(\n",
    "            self.cfg.DATA.PATH_TO_DATA_DIR,\n",
    "            \"{}.csv\".format(\"train\" if self.mode == \"train\" else \"val\"),\n",
    "        )\n",
    "        assert g_pathmgr.exists(path_to_file), \"{} dir not found\".format(\n",
    "            path_to_file\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        (self._path_to_videos, self._labels) = utils.load_image_lists(\n",
    "            path_to_file, self.cfg.DATA.PATH_PREFIX, return_list=True\n",
    "        )\n",
    "\n",
    "        if self.mode != \"train\":\n",
    "            # Form video-level labels from frame level annotations.\n",
    "            self._labels = utils.convert_to_video_level_labels(self._labels)\n",
    "\n",
    "        self._path_to_videos = list(\n",
    "            chain.from_iterable(\n",
    "                [[x] * self._num_clips for x in self._path_to_videos]\n",
    "            )\n",
    "        )\n",
    "        self._labels = list(\n",
    "            chain.from_iterable([[x] * self._num_clips for x in self._labels])\n",
    "        )\n",
    "        self._spatial_temporal_idx = list(\n",
    "            chain.from_iterable(\n",
    "                [range(self._num_clips) for _ in range(len(self._labels))]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        logger.info(\n",
    "            \"Charades dataloader constructed (size: {}) from {}\".format(\n",
    "                len(self._path_to_videos), path_to_file\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_seq_frames(self, index):\n",
    "        \"\"\"\n",
    "        Given the video index, return the list of indexs of sampled frames.\n",
    "        Args:\n",
    "            index (int): the video index.\n",
    "        Returns:\n",
    "            seq (list): the indexes of sampled frames from the video.\n",
    "        \"\"\"\n",
    "        temporal_sample_index = (\n",
    "            -1\n",
    "            if self.mode in [\"train\", \"val\"]\n",
    "            else self._spatial_temporal_idx[index]\n",
    "            // self.cfg.TEST.NUM_SPATIAL_CROPS\n",
    "        )\n",
    "        num_frames = self.cfg.DATA.NUM_FRAMES\n",
    "        sampling_rate = utils.get_random_sampling_rate(\n",
    "            self.cfg.MULTIGRID.LONG_CYCLE_SAMPLING_RATE,\n",
    "            self.cfg.DATA.SAMPLING_RATE,\n",
    "        )\n",
    "        video_length = len(self._path_to_videos[index])\n",
    "        assert video_length == len(self._labels[index])\n",
    "\n",
    "        clip_length = (num_frames - 1) * sampling_rate + 1\n",
    "        if temporal_sample_index == -1:\n",
    "            if clip_length > video_length:\n",
    "                start = random.randint(video_length - clip_length, 0)\n",
    "            else:\n",
    "                start = random.randint(0, video_length - clip_length)\n",
    "        else:\n",
    "            gap = float(max(video_length - clip_length, 0)) / (\n",
    "                self.cfg.TEST.NUM_ENSEMBLE_VIEWS - 1\n",
    "            )\n",
    "            start = int(round(gap * temporal_sample_index))\n",
    "\n",
    "        seq = [\n",
    "            max(min(start + i * sampling_rate, video_length - 1), 0)\n",
    "            for i in range(num_frames)\n",
    "        ]\n",
    "\n",
    "        return seq\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Given the video index, return the list of frames, label, and video\n",
    "        index if the video frames can be fetched.\n",
    "        Args:\n",
    "            index (int): the video index provided by the pytorch sampler.\n",
    "        Returns:\n",
    "            frames (tensor): the frames of sampled from the video. The dimension\n",
    "                is `channel` x `num frames` x `height` x `width`.\n",
    "            label (int): the label of the current video.\n",
    "            index (int): the index of the video.\n",
    "        \"\"\"\n",
    "        short_cycle_idx = None\n",
    "        # When short cycle is used, input index is a tupple.\n",
    "        if isinstance(index, tuple):\n",
    "            index, short_cycle_idx = index\n",
    "\n",
    "        if self.mode in [\"train\", \"val\"]:\n",
    "            # -1 indicates random sampling.\n",
    "            spatial_sample_index = -1\n",
    "            min_scale = self.cfg.DATA.TRAIN_JITTER_SCALES[0]\n",
    "            max_scale = self.cfg.DATA.TRAIN_JITTER_SCALES[1]\n",
    "            crop_size = self.cfg.DATA.TRAIN_CROP_SIZE\n",
    "            if short_cycle_idx in [0, 1]:\n",
    "                crop_size = int(\n",
    "                    round(\n",
    "                        self.cfg.MULTIGRID.SHORT_CYCLE_FACTORS[short_cycle_idx]\n",
    "                        * self.cfg.MULTIGRID.DEFAULT_S\n",
    "                    )\n",
    "                )\n",
    "            if self.cfg.MULTIGRID.DEFAULT_S > 0:\n",
    "                # Decreasing the scale is equivalent to using a larger \"span\"\n",
    "                # in a sampling grid.\n",
    "                min_scale = int(\n",
    "                    round(\n",
    "                        float(min_scale)\n",
    "                        * crop_size\n",
    "                        / self.cfg.MULTIGRID.DEFAULT_S\n",
    "                    )\n",
    "                )\n",
    "        elif self.mode in [\"test\"]:\n",
    "            # spatial_sample_index is in [0, 1, 2]. Corresponding to left,\n",
    "            # center, or right if width is larger than height, and top, middle,\n",
    "            # or bottom if height is larger than width.\n",
    "            spatial_sample_index = (\n",
    "                self._spatial_temporal_idx[index]\n",
    "                % self.cfg.TEST.NUM_SPATIAL_CROPS\n",
    "            )\n",
    "            min_scale, max_scale, crop_size = [self.cfg.DATA.TEST_CROP_SIZE] * 3\n",
    "            # The testing is deterministic and no jitter should be performed.\n",
    "            # min_scale, max_scale, and crop_size are expect to be the same.\n",
    "            assert len({min_scale, max_scale, crop_size}) == 1\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"Does not support {} mode\".format(self.mode)\n",
    "            )\n",
    "\n",
    "        seq = self.get_seq_frames(index)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        frames = torch.as_tensor(\n",
    "            utils.retry_load_images(\n",
    "                [self._path_to_videos[index][frame] for frame in seq],\n",
    "                self._num_retries,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        label = utils.aggregate_labels(\n",
    "            [self._labels[index][i] for i in range(seq[0], seq[-1] + 1)]\n",
    "        )\n",
    "        label = torch.as_tensor(\n",
    "            utils.as_binary_vector(label, self.cfg.MODEL.NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "        # Perform color normalization.\n",
    "        frames = utils.tensor_normalize(\n",
    "            frames, self.cfg.DATA.MEAN, self.cfg.DATA.STD\n",
    "        )\n",
    "        # T H W C -> C T H W.\n",
    "        frames = frames.permute(3, 0, 1, 2)\n",
    "        # Perform data augmentation.\n",
    "        frames = utils.spatial_sampling(\n",
    "            frames,\n",
    "            spatial_idx=spatial_sample_index,\n",
    "            min_scale=min_scale,\n",
    "            max_scale=max_scale,\n",
    "            crop_size=crop_size,\n",
    "            random_horizontal_flip=self.cfg.DATA.RANDOM_FLIP,\n",
    "            inverse_uniform_sampling=self.cfg.DATA.INV_UNIFORM_SAMPLE,\n",
    "        )\n",
    "        frames = utils.pack_pathway_output(self.cfg, frames)\n",
    "        return frames, label, index, {}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (int): the number of videos in the dataset.\n",
    "        \"\"\"\n",
    "        return self.num_videos\n",
    "\n",
    "    @property\n",
    "    def num_videos(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            (int): the number of videos in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self._path_to_videos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
